#!/usr/bin/env python3
"""
Test Results Aggregator for Jarvis-V0.19
Automatically scans, analyzes and aggregates all test files/logs generated by the system.
"""

import os
import json
import glob
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict, Counter
import statistics

class TestAggregator:
    """Comprehensive test results aggregation and analysis system."""
    
    def __init__(self, base_path: str = None):
        self.base_path = Path(base_path) if base_path else Path(__file__).parent.parent
        self.logs_dir = self.base_path / "logs"
        self.data_dir = self.base_path / "data"
        self.agent_reports_dir = self.data_dir / "agent_reports"
        self.archive_db = self.data_dir / "jarvis_archive.db"
        
        # Unified test output directory
        self.test_output_dir = self.base_path / "tests" / "output"
        self.test_reports_dir = self.test_output_dir / "reports"
        self.test_logs_dir = self.test_output_dir / "logs"
        self.test_agent_reports_dir = self.test_output_dir / "agent_reports"
        
        # Ensure output directories exist
        self.test_output_dir.mkdir(parents=True, exist_ok=True)
        self.test_reports_dir.mkdir(parents=True, exist_ok=True)
        self.test_logs_dir.mkdir(parents=True, exist_ok=True)
        self.test_agent_reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize aggregated data structure
        self.aggregated_data = {
            "scan_timestamp": datetime.now().isoformat(),
            "file_stats": {},
            "test_results": {},
            "performance_metrics": {},
            "error_analysis": {},
            "archive_stats": {},
            "agent_activity": {},
            "recommendations": [],
            "summary": {}
        }
        
    def scan_all_files(self) -> Dict[str, List[str]]:
        """Scan all directories for test files and categorize them."""
        print(f"[SCAN] Scanning test files in {self.base_path}")
        
        file_categories = {
            "function_tests": [],
            "performance_logs": [],
            "concurrent_logs": [],
            "large_event_logs": [],
            "workflow_logs": [],
            "test_events": [],
            "agent_reports": [],
            "error_logs": [],
            "archive_data": []
        }
        
        # Scan unified test output directories (new location)
        test_logs_dir = self.base_path / "tests" / "output" / "logs"
        if test_logs_dir.exists():
            for pattern, category in [
                ("function_test_results_*.json", "function_tests"),
                ("perf_event_*.json", "performance_logs"),
                ("concurrent_log_*.json", "concurrent_logs"),
                ("large_event_*.json", "large_event_logs"),
                ("workflow_event_*.json", "workflow_logs"),
                ("test_event_*.json", "test_events")
            ]:
                files = list(test_logs_dir.glob(pattern))
                file_categories[category].extend([str(f) for f in files])
        
        # Scan original logs directory (for compatibility)
        if self.logs_dir.exists():
            for pattern, category in [
                ("function_test_results_*.json", "function_tests"),
                ("perf_event_*.json", "performance_logs"),
                ("concurrent_log_*.json", "concurrent_logs"),
                ("large_event_*.json", "large_event_logs"),
                ("workflow_event_*.json", "workflow_logs"),
                ("test_event_*.json", "test_events"),
                ("error_log*.jsonl", "error_logs")
            ]:
                files = list(self.logs_dir.glob(pattern))
                file_categories[category].extend([str(f) for f in files])
        
        # Scan unified test agent reports (new location)
        test_agent_reports_dir = self.base_path / "tests" / "output" / "agent_reports"
        if test_agent_reports_dir.exists():
            agent_files = list(test_agent_reports_dir.glob("*.json"))
            file_categories["agent_reports"].extend([str(f) for f in agent_files])
        
        # Scan original agent reports (for compatibility)
        if self.agent_reports_dir.exists():
            agent_files = list(self.agent_reports_dir.glob("*.json"))
            file_categories["agent_reports"].extend([str(f) for f in agent_files])
        
        # Archive database
        if self.archive_db.exists():
            file_categories["archive_data"].append(str(self.archive_db))
        
        # Calculate file statistics
        total_files = sum(len(files) for files in file_categories.values())
        total_size = 0
        
        for category, files in file_categories.items():
            category_size = 0
            for file_path in files:
                try:
                    category_size += Path(file_path).stat().st_size
                except:
                    pass
            total_size += category_size
            
            self.aggregated_data["file_stats"][category] = {
                "count": len(files),
                "size_bytes": category_size,
                "size_mb": round(category_size / (1024*1024), 2)
            }
        
        self.aggregated_data["file_stats"]["totals"] = {
            "total_files": total_files,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024*1024), 2)
        }
        
        print(f"[INFO] Found {total_files} files ({round(total_size/(1024*1024), 1)}MB)")
        return file_categories
    
    def analyze_function_tests(self, files: List[str]) -> Dict[str, Any]:
        """Analyze function test results."""
        print("[ANALYZE] Processing function test results...")
        
        results = {
            "total_tests": 0,
            "successful_tests": 0,
            "failed_tests": 0,
            "success_rates": [],
            "latest_results": None,
            "test_categories": defaultdict(int),
            "status_distribution": Counter()
        }
        
        for file_path in files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                
                if "summary" in data:
                    summary = data["summary"]
                    results["total_tests"] += summary.get("total", 0)
                    results["successful_tests"] += summary.get("successful", 0)
                    results["failed_tests"] += summary.get("failed", 0)
                    
                    if "success_rate" in summary:
                        results["success_rates"].append(summary["success_rate"])
                    
                    # Track latest results
                    if not results["latest_results"] or data.get("timestamp", "") > results["latest_results"].get("timestamp", ""):
                        results["latest_results"] = data
                
                # Analyze detailed results
                if "detailed_results" in data:
                    for category, items in data["detailed_results"].items():
                        results["test_categories"][category] += len(items) if isinstance(items, list) else 1
                        
                        if isinstance(items, list):
                            for item in items:
                                if isinstance(item, dict) and "status" in item:
                                    results["status_distribution"][item["status"]] += 1
                
            except Exception as e:
                print(f"[WARN] Error processing {file_path}: {e}")
        
        # Calculate averages
        if results["success_rates"]:
            results["average_success_rate"] = statistics.mean(results["success_rates"])
            results["min_success_rate"] = min(results["success_rates"])
            results["max_success_rate"] = max(results["success_rates"])
        
        return results
    
    def analyze_performance_logs(self, files: List[str]) -> Dict[str, Any]:
        """Analyze performance test logs."""
        print(f"[ANALYZE] Processing {len(files)} performance logs...")
        
        metrics = {
            "total_events": 0,
            "time_range": {"start": None, "end": None},
            "event_types": Counter(),
            "performance_data": [],
            "latency_stats": {"min": float('inf'), "max": 0, "avg": 0},
            "throughput_estimate": 0
        }
        
        timestamps = []
        
        for file_path in files[:100]:  # Sample first 100 files for performance
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                
                metrics["total_events"] += 1
                
                if "timestamp" in data:
                    timestamps.append(data["timestamp"])
                
                if "event" in data:
                    metrics["event_types"][data["event"]] += 1
                
                # Extract performance timing if available
                if "data" in data and isinstance(data["data"], dict):
                    event_data = data["data"]
                    if "timestamp" in event_data:
                        event_time = event_data["timestamp"]
                        metrics["performance_data"].append(event_time)
                
            except Exception as e:
                print(f"[WARN] Error processing performance log {file_path}: {e}")
        
        # Calculate time range and throughput
        if timestamps:
            timestamps.sort()
            metrics["time_range"]["start"] = timestamps[0]
            metrics["time_range"]["end"] = timestamps[-1]
            
            # Estimate throughput (events per second)
            try:
                start_dt = datetime.fromisoformat(timestamps[0].replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(timestamps[-1].replace('Z', '+00:00'))
                duration = (end_dt - start_dt).total_seconds()
                if duration > 0:
                    metrics["throughput_estimate"] = round(len(files) / duration, 2)
            except:
                pass
        
        return metrics
    
    def analyze_concurrent_logs(self, files: List[str]) -> Dict[str, Any]:
        """Analyze concurrent operation logs."""
        print(f"[ANALYZE] Processing {len(files)} concurrent logs...")
        
        analysis = {
            "total_concurrent_events": len(files),
            "worker_distribution": Counter(),
            "event_distribution": Counter(),
            "concurrency_patterns": {}
        }
        
        # Sample analysis of first 50 files
        for file_path in files[:50]:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                
                if "event" in data and "data" in data:
                    event_data = data["data"]
                    
                    if "worker_id" in event_data:
                        analysis["worker_distribution"][event_data["worker_id"]] += 1
                    
                    if "event_index" in event_data:
                        analysis["event_distribution"][event_data["event_index"]] += 1
                
            except Exception as e:
                print(f"[WARN] Error processing concurrent log {file_path}: {e}")
        
        # Analyze concurrency patterns
        worker_counts = list(analysis["worker_distribution"].values())
        if worker_counts:
            analysis["concurrency_patterns"]["max_worker_events"] = max(worker_counts)
            analysis["concurrency_patterns"]["min_worker_events"] = min(worker_counts)
            analysis["concurrency_patterns"]["avg_worker_events"] = statistics.mean(worker_counts)
            analysis["concurrency_patterns"]["total_workers"] = len(analysis["worker_distribution"])
        
        return analysis
    
    def analyze_archive_database(self) -> Dict[str, Any]:
        """Analyze the archive database."""
        print("[ANALYZE] Processing archive database...")
        
        archive_stats = {
            "total_entries": 0,
            "entry_types": Counter(),
            "status_distribution": Counter(),
            "verification_stats": {},
            "temporal_analysis": {}
        }
        
        try:
            conn = sqlite3.connect(self.archive_db)
            cursor = conn.cursor()
            
            # Total entries
            cursor.execute("SELECT COUNT(*) FROM archive_entries")
            archive_stats["total_entries"] = cursor.fetchone()[0]
            
            # Entry types
            cursor.execute("SELECT entry_type, COUNT(*) FROM archive_entries GROUP BY entry_type")
            for entry_type, count in cursor.fetchall():
                archive_stats["entry_types"][entry_type] = count
            
            # Status distribution
            cursor.execute("SELECT status, COUNT(*) FROM archive_entries GROUP BY status")
            for status, count in cursor.fetchall():
                archive_stats["status_distribution"][status] = count
            
            # Verification statistics
            cursor.execute("SELECT AVG(confidence_score), MIN(confidence_score), MAX(confidence_score) FROM archive_entries WHERE confidence_score IS NOT NULL")
            conf_stats = cursor.fetchone()
            if conf_stats and conf_stats[0] is not None:
                archive_stats["verification_stats"] = {
                    "avg_confidence": round(conf_stats[0], 3),
                    "min_confidence": conf_stats[1],
                    "max_confidence": conf_stats[2]
                }
            
            # Recent activity (last 24 hours)
            cursor.execute("""
                SELECT COUNT(*) FROM archive_entries 
                WHERE created_at > datetime('now', '-1 day')
            """)
            archive_stats["temporal_analysis"]["last_24h_entries"] = cursor.fetchone()[0]
            
            conn.close()
            
        except Exception as e:
            print(f"[WARN] Error analyzing archive database: {e}")
            archive_stats["error"] = str(e)
        
        return archive_stats
    
    def analyze_agent_reports(self, files: List[str]) -> Dict[str, Any]:
        """Analyze agent activity reports."""
        print(f"[ANALYZE] Processing {len(files)} agent reports...")
        
        agent_analysis = {
            "total_reports": len(files),
            "agent_performance": {},
            "success_trends": [],
            "critical_issues": [],
            "recommendations": []
        }
        
        for file_path in files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                
                agent_id = data.get("agent_id", "unknown")
                if agent_id not in agent_analysis["agent_performance"]:
                    agent_analysis["agent_performance"][agent_id] = {
                        "total_cycles": 0,
                        "successful_cycles": 0,
                        "average_scores": [],
                        "compliance_rates": []
                    }
                
                perf = agent_analysis["agent_performance"][agent_id]
                perf["total_cycles"] += data.get("total_cycles", 0)
                perf["successful_cycles"] += data.get("successful_cycles", 0)
                
                if "average_score" in data:
                    perf["average_scores"].append(data["average_score"])
                
                if "compliance_rate" in data:
                    perf["compliance_rates"].append(data["compliance_rate"])
                
                # Collect critical issues
                if "critical_issues" in data:
                    agent_analysis["critical_issues"].extend(data["critical_issues"])
                
                # Collect recommendations
                if "recommendations" in data:
                    agent_analysis["recommendations"].extend(data["recommendations"])
                
            except Exception as e:
                print(f"[WARN] Error processing agent report {file_path}: {e}")
        
        # Calculate aggregate statistics
        for agent_id, perf in agent_analysis["agent_performance"].items():
            if perf["average_scores"]:
                perf["overall_avg_score"] = statistics.mean(perf["average_scores"])
            if perf["compliance_rates"]:
                perf["overall_compliance"] = statistics.mean(perf["compliance_rates"])
        
        return agent_analysis
    
    def analyze_error_logs(self, files: List[str]) -> Dict[str, Any]:
        """Analyze error logs."""
        print(f"[ANALYZE] Processing {len(files)} error log files...")
        
        error_analysis = {
            "total_log_files": len(files),
            "error_categories": Counter(),
            "error_levels": Counter(),
            "recent_errors": [],
            "error_trends": {}
        }
        
        for file_path in files:
            try:
                with open(file_path, 'r') as f:
                    for line_num, line in enumerate(f):
                        if line.strip():
                            try:
                                log_entry = json.loads(line.strip())
                                
                                # Categorize errors
                                if "level" in log_entry:
                                    error_analysis["error_levels"][log_entry["level"]] += 1
                                
                                if "message" in log_entry:
                                    msg = log_entry["message"].lower()
                                    if "unicode" in msg:
                                        error_analysis["error_categories"]["UnicodeError"] += 1
                                    elif "test" in msg:
                                        error_analysis["error_categories"]["TestError"] += 1
                                    elif "import" in msg or "module" in msg:
                                        error_analysis["error_categories"]["ImportError"] += 1
                                    else:
                                        error_analysis["error_categories"]["Other"] += 1
                                
                                # Keep recent errors for analysis
                                if len(error_analysis["recent_errors"]) < 10:
                                    error_analysis["recent_errors"].append({
                                        "timestamp": log_entry.get("timestamp", "unknown"),
                                        "level": log_entry.get("level", "unknown"),
                                        "message": log_entry.get("message", "")[:100] + "..." if len(log_entry.get("message", "")) > 100 else log_entry.get("message", "")
                                    })
                                
                            except json.JSONDecodeError:
                                continue
                                
            except Exception as e:
                print(f"[WARN] Error processing error log {file_path}: {e}")
        
        return error_analysis
    
    def generate_recommendations(self) -> List[str]:
        """Generate recommendations based on analysis."""
        recommendations = []
        
        # Test results recommendations
        test_results = self.aggregated_data.get("test_results", {})
        if test_results.get("average_success_rate", 100) < 95:
            recommendations.append("PRIORITY: Test success rate below 95% - investigate failing tests")
        
        # Performance recommendations
        perf_metrics = self.aggregated_data.get("performance_metrics", {})
        if perf_metrics.get("total_events", 0) > 10000:
            recommendations.append("INFO: High volume of performance logs detected - consider log rotation")
        
        # Archive recommendations
        archive_stats = self.aggregated_data.get("archive_stats", {})
        if archive_stats.get("status_distribution", {}).get("rejected", 0) > 100:
            recommendations.append("ATTENTION: High number of rejected archive entries - review verification process")
        
        # File management recommendations
        file_stats = self.aggregated_data.get("file_stats", {})
        total_size_mb = file_stats.get("totals", {}).get("total_size_mb", 0)
        if total_size_mb > 100:
            recommendations.append(f"MAINTENANCE: Large log files detected ({total_size_mb}MB) - consider cleanup")
        
        # Error analysis recommendations
        error_analysis = self.aggregated_data.get("error_analysis", {})
        unicode_errors = error_analysis.get("error_categories", {}).get("UnicodeError", 0)
        if unicode_errors > 5:
            recommendations.append("FIX: Multiple Unicode errors detected - ensure proper encoding handling")
        
        # Agent performance recommendations
        agent_activity = self.aggregated_data.get("agent_activity", {})
        for agent_id, perf in agent_activity.get("agent_performance", {}).items():
            if perf.get("overall_compliance", 0) < 0.8:
                recommendations.append(f"REVIEW: Agent {agent_id} compliance rate below 80%")
        
        if not recommendations:
            recommendations.append("EXCELLENT: All systems operating within normal parameters")
        
        return recommendations
    
    def generate_summary(self) -> Dict[str, Any]:
        """Generate overall summary."""
        summary = {
            "overall_status": "UNKNOWN",
            "key_metrics": {},
            "health_score": 0,
            "critical_issues": 0,
            "total_data_points": 0
        }
        
        # Calculate total data points
        file_stats = self.aggregated_data.get("file_stats", {})
        summary["total_data_points"] = file_stats.get("totals", {}).get("total_files", 0)
        
        # Test health
        test_results = self.aggregated_data.get("test_results", {})
        test_success_rate = test_results.get("average_success_rate", 100)
        summary["key_metrics"]["test_success_rate"] = f"{test_success_rate:.1f}%"
        
        # Archive health
        archive_stats = self.aggregated_data.get("archive_stats", {})
        summary["key_metrics"]["archive_entries"] = archive_stats.get("total_entries", 0)
        
        # Performance metrics
        perf_metrics = self.aggregated_data.get("performance_metrics", {})
        summary["key_metrics"]["performance_events"] = perf_metrics.get("total_events", 0)
        summary["key_metrics"]["throughput"] = f"{perf_metrics.get('throughput_estimate', 0)} events/sec"
        
        # Error analysis
        error_analysis = self.aggregated_data.get("error_analysis", {})
        total_errors = sum(error_analysis.get("error_levels", {}).values())
        summary["key_metrics"]["total_errors"] = total_errors
        
        # Calculate health score (0-100) - only count actual "error" level entries
        health_score = 100
        if test_success_rate < 100:
            health_score -= (100 - test_success_rate) * 2
        
        # Only count actual "error" level entries, not warnings or info
        actual_errors = error_analysis.get("error_levels", {}).get("error", 0)
        if actual_errors > 5:
            health_score -= min(actual_errors - 5, 15)
        
        summary["health_score"] = max(0, min(100, health_score))
        
        # Overall status
        if summary["health_score"] >= 95:
            summary["overall_status"] = "EXCELLENT"
        elif summary["health_score"] >= 80:
            summary["overall_status"] = "GOOD"
        elif summary["health_score"] >= 60:
            summary["overall_status"] = "WARNING"
        else:
            summary["overall_status"] = "CRITICAL"
        
        return summary
    
    def run_aggregation(self) -> Dict[str, Any]:
        """Run complete aggregation process."""
        print("=" * 80)
        print("[LAUNCH] JARVIS TEST AGGREGATION SYSTEM")
        print("=" * 80)
        
        # 1. Scan all files
        file_categories = self.scan_all_files()
        
        # 2. Analyze each category
        self.aggregated_data["test_results"] = self.analyze_function_tests(
            file_categories["function_tests"]
        )
        
        self.aggregated_data["performance_metrics"] = self.analyze_performance_logs(
            file_categories["performance_logs"]
        )
        
        self.aggregated_data["concurrent_analysis"] = self.analyze_concurrent_logs(
            file_categories["concurrent_logs"]
        )
        
        self.aggregated_data["archive_stats"] = self.analyze_archive_database()
        
        self.aggregated_data["agent_activity"] = self.analyze_agent_reports(
            file_categories["agent_reports"]
        )
        
        self.aggregated_data["error_analysis"] = self.analyze_error_logs(
            file_categories["error_logs"]
        )
        
        # 3. Generate recommendations and summary
        self.aggregated_data["recommendations"] = self.generate_recommendations()
        self.aggregated_data["summary"] = self.generate_summary()
        
        print(f"\n[COMPLETE] Aggregation finished - Health Score: {self.aggregated_data['summary']['health_score']}/100")
        return self.aggregated_data
    
    def save_json_report(self, output_path: str = None) -> str:
        """Save detailed JSON report to unified test output directory."""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.test_reports_dir / f"TEST_AGGREGATE_REPORT_{timestamp}.json"
        
        with open(output_path, 'w') as f:
            json.dump(self.aggregated_data, f, indent=2, default=str)
        
        return str(output_path)
    
    def save_markdown_report(self, output_path: str = None) -> str:
        """Save human-readable markdown report to unified test output directory."""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.test_reports_dir / f"TEST_AGGREGATE_REPORT_{timestamp}.md"
        
        summary = self.aggregated_data["summary"]
        file_stats = self.aggregated_data["file_stats"]
        test_results = self.aggregated_data["test_results"]
        perf_metrics = self.aggregated_data["performance_metrics"]
        archive_stats = self.aggregated_data["archive_stats"]
        agent_activity = self.aggregated_data["agent_activity"]
        error_analysis = self.aggregated_data["error_analysis"]
        recommendations = self.aggregated_data["recommendations"]
        
        report = f"""# Jarvis-V0.19 Test Aggregation Report

**Generated:** {self.aggregated_data['scan_timestamp']}  
**Overall Status:** {summary['overall_status']}  
**Health Score:** {summary['health_score']}/100

## Executive Summary

{summary['key_metrics']['test_success_rate']} test success rate across {summary['total_data_points']} data files
- **Archive Entries:** {summary['key_metrics']['archive_entries']} total entries
- **Performance Events:** {summary['key_metrics']['performance_events']} logged
- **System Throughput:** {summary['key_metrics']['throughput']}
- **Total Errors:** {summary['key_metrics']['total_errors']}

## File Analysis Summary

| Category | Files | Size (MB) |
|----------|-------|-----------|
| Function Tests | {file_stats.get('function_tests', {}).get('count', 0)} | {file_stats.get('function_tests', {}).get('size_mb', 0)} |
| Performance Logs | {file_stats.get('performance_logs', {}).get('count', 0)} | {file_stats.get('performance_logs', {}).get('size_mb', 0)} |
| Concurrent Logs | {file_stats.get('concurrent_logs', {}).get('count', 0)} | {file_stats.get('concurrent_logs', {}).get('size_mb', 0)} |
| Agent Reports | {file_stats.get('agent_reports', {}).get('count', 0)} | {file_stats.get('agent_reports', {}).get('size_mb', 0)} |
| Error Logs | {file_stats.get('error_logs', {}).get('count', 0)} | {file_stats.get('error_logs', {}).get('size_mb', 0)} |
| **TOTAL** | **{file_stats.get('totals', {}).get('total_files', 0)}** | **{file_stats.get('totals', {}).get('total_size_mb', 0)}** |

## Test Results Analysis

### Function Tests
- **Total Tests:** {test_results.get('total_tests', 0)}
- **Success Rate:** {test_results.get('average_success_rate', 0):.1f}% (min: {test_results.get('min_success_rate', 0):.1f}%, max: {test_results.get('max_success_rate', 0):.1f}%)
- **Successful:** {test_results.get('successful_tests', 0)}
- **Failed:** {test_results.get('failed_tests', 0)}

### Test Categories
"""

        # Add test categories
        for category, count in test_results.get('test_categories', {}).items():
            report += f"- **{category}:** {count} tests\n"

        report += f"""

### Status Distribution
"""
        for status, count in test_results.get('status_distribution', {}).items():
            report += f"- **{status}:** {count}\n"

        report += f"""

## Performance Analysis

### Metrics Overview
- **Total Events:** {perf_metrics.get('total_events', 0):,}
- **Event Types:** {len(perf_metrics.get('event_types', {}))} different types
- **Estimated Throughput:** {perf_metrics.get('throughput_estimate', 0)} events/second

### Event Distribution
"""
        
        for event_type, count in perf_metrics.get('event_types', {}).most_common(10):
            report += f"- **{event_type}:** {count:,} events\n"

        report += f"""

## Archive Database Analysis

- **Total Entries:** {archive_stats.get('total_entries', 0):,}
- **Recent Activity (24h):** {archive_stats.get('temporal_analysis', {}).get('last_24h_entries', 0)} entries

### Entry Types
"""
        for entry_type, count in archive_stats.get('entry_types', {}).items():
            report += f"- **{entry_type}:** {count:,}\n"

        report += f"""

### Status Distribution
"""
        for status, count in archive_stats.get('status_distribution', {}).items():
            report += f"- **{status}:** {count:,}\n"

        if 'verification_stats' in archive_stats:
            vs = archive_stats['verification_stats']
            report += f"""

### Verification Statistics
- **Average Confidence:** {vs.get('avg_confidence', 0):.3f}
- **Confidence Range:** {vs.get('min_confidence', 0):.3f} - {vs.get('max_confidence', 0):.3f}
"""

        report += f"""

## Agent Activity Analysis

- **Total Reports:** {agent_activity.get('total_reports', 0)}
- **Active Agents:** {len(agent_activity.get('agent_performance', {}))}

### Agent Performance
"""
        
        for agent_id, perf in agent_activity.get('agent_performance', {}).items():
            compliance = perf.get('overall_compliance', 0)
            score = perf.get('overall_avg_score', 0)
            report += f"- **{agent_id}:**\n"
            report += f"  - Cycles: {perf.get('total_cycles', 0)} total, {perf.get('successful_cycles', 0)} successful\n"
            report += f"  - Compliance: {compliance:.1%}\n"
            report += f"  - Average Score: {score:.2f}\n"

        report += f"""

## Error Analysis

- **Total Error Events:** {sum(error_analysis.get('error_levels', {}).values())}
- **Error Categories:** {len(error_analysis.get('error_categories', {}))}

### Error Categories
"""
        for category, count in error_analysis.get('error_categories', {}).items():
            report += f"- **{category}:** {count}\n"

        report += f"""

### Error Levels
"""
        for level, count in error_analysis.get('error_levels', {}).items():
            report += f"- **{level}:** {count}\n"

        report += f"""

## Recommendations

"""
        for i, rec in enumerate(recommendations, 1):
            report += f"{i}. {rec}\n"

        report += f"""

## Technical Details

### Concurrent Operations Analysis
- **Total Concurrent Events:** {self.aggregated_data.get('concurrent_analysis', {}).get('total_concurrent_events', 0):,}
- **Active Workers:** {self.aggregated_data.get('concurrent_analysis', {}).get('concurrency_patterns', {}).get('total_workers', 0)}
- **Average Events per Worker:** {self.aggregated_data.get('concurrent_analysis', {}).get('concurrency_patterns', {}).get('avg_worker_events', 0):.1f}

### Critical Issues Identified
"""
        
        critical_issues = agent_activity.get('critical_issues', [])
        if critical_issues:
            for issue in critical_issues[:5]:  # Show max 5 issues
                report += f"- {issue}\n"
        else:
            report += "- No critical issues detected\n"

        report += f"""

### System Health Indicators
- **Overall System Health:** {summary['health_score']}/100
- **Data Completeness:** {"COMPLETE" if summary['total_data_points'] > 0 else "INCOMPLETE"}
- **Test Coverage:** {"COMPREHENSIVE" if test_results.get('total_tests', 0) > 50 else "BASIC"}
- **Performance Monitoring:** {"ACTIVE" if perf_metrics.get('total_events', 0) > 1000 else "LIMITED"}

---

*This report was automatically generated by the Jarvis-V0.19 Test Aggregation System*
"""

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return str(output_path)


def main():
    """Main entry point for the test aggregator."""
    print("Jarvis-V0.19 Test Aggregation System")
    print("====================================")
    
    # Initialize aggregator
    aggregator = TestAggregator()
    
    # Run aggregation
    results = aggregator.run_aggregation()
    
    # Save reports
    json_path = aggregator.save_json_report()
    md_path = aggregator.save_markdown_report()
    
    print(f"\n[SAVE] Reports generated:")
    print(f"  JSON: {json_path}")
    print(f"  Markdown: {md_path}")
    
    # Display summary
    summary = results["summary"]
    print(f"\n[SUMMARY] System Health: {summary['overall_status']} ({summary['health_score']}/100)")
    print(f"[INFO] Analyzed {summary['total_data_points']} files")
    print(f"[INFO] {len(results['recommendations'])} recommendations generated")
    
    return results


if __name__ == "__main__":
    main()
"""
CRDT Enhanced Monitoring Dashboard
Phase 5 - Advanced Features: Comprehensive monitoring and observability

This module extends the system dashboard with advanced CRDT monitoring,
real-time synchronization visualization, and enterprise-grade reporting.
"""

import json
import time
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import threading
from collections import defaultdict, deque
import statistics

logger = logging.getLogger(__name__)


@dataclass
class CRDTHealthMetrics:
    """Comprehensive CRDT health metrics"""
    timestamp: datetime
    sync_status: str
    active_peers: int
    total_operations: int
    successful_syncs: int
    failed_syncs: int
    conflicts_detected: int
    conflicts_resolved: int
    average_sync_time_ms: float
    network_partition_resilience: float
    data_consistency_score: float
    performance_impact_percent: float


@dataclass
class SyncMetrics:
    """Synchronization performance metrics"""
    peer_node: str
    sync_duration_ms: float
    operations_sent: int
    operations_received: int
    bandwidth_used_bytes: int
    compression_ratio: float
    success: bool
    timestamp: datetime
    error_message: Optional[str] = None


@dataclass
class ConflictMetrics:
    """Conflict resolution metrics"""
    conflict_id: str
    conflict_type: str
    detection_time: datetime
    resolution_time: Optional[datetime]
    resolution_strategy: str
    involved_nodes: List[str]
    resolution_duration_ms: Optional[float]
    success: bool
    manual_intervention: bool


class CRDTMetricsCollector:
    """Collect and aggregate CRDT metrics"""
    
    def __init__(self, max_history: int = 1000):
        self.max_history = max_history
        self.health_metrics = deque(maxlen=max_history)
        self.sync_metrics = deque(maxlen=max_history)
        self.conflict_metrics = deque(maxlen=max_history)
        self.performance_baseline = None
        self._lock = threading.RLock()
        
    def record_health_metrics(self, metrics: CRDTHealthMetrics):
        """Record health metrics"""
        with self._lock:
            self.health_metrics.append(metrics)
    
    def record_sync_metrics(self, metrics: SyncMetrics):
        """Record synchronization metrics"""
        with self._lock:
            self.sync_metrics.append(metrics)
    
    def record_conflict_metrics(self, metrics: ConflictMetrics):
        """Record conflict metrics"""
        with self._lock:
            self.conflict_metrics.append(metrics)
    
    def get_current_health_score(self) -> float:
        """Calculate current CRDT health score"""
        if not self.health_metrics:
            return 100.0
        
        recent_metrics = list(self.health_metrics)[-10:]  # Last 10 measurements
        
        # Component scores
        sync_score = self._calculate_sync_score(recent_metrics)
        conflict_score = self._calculate_conflict_score()
        performance_score = self._calculate_performance_score(recent_metrics)
        consistency_score = self._calculate_consistency_score(recent_metrics)
        
        # Weighted overall score
        weights = {
            'sync': 0.3,
            'conflict': 0.2,
            'performance': 0.3,
            'consistency': 0.2
        }
        
        overall_score = (
            sync_score * weights['sync'] +
            conflict_score * weights['conflict'] +
            performance_score * weights['performance'] +
            consistency_score * weights['consistency']
        )
        
        return min(100.0, max(0.0, overall_score))
    
    def _calculate_sync_score(self, recent_metrics: List[CRDTHealthMetrics]) -> float:
        """Calculate synchronization health score"""
        if not recent_metrics:
            return 100.0
        
        total_syncs = sum(m.successful_syncs + m.failed_syncs for m in recent_metrics)
        if total_syncs == 0:
            return 100.0
        
        successful_syncs = sum(m.successful_syncs for m in recent_metrics)
        success_rate = successful_syncs / total_syncs
        
        return success_rate * 100
    
    def _calculate_conflict_score(self) -> float:
        """Calculate conflict resolution health score"""
        if not self.conflict_metrics:
            return 100.0
        
        recent_conflicts = [c for c in self.conflict_metrics 
                          if c.detection_time > datetime.utcnow() - timedelta(hours=1)]
        
        if not recent_conflicts:
            return 100.0
        
        resolved_conflicts = sum(1 for c in recent_conflicts if c.success)
        resolution_rate = resolved_conflicts / len(recent_conflicts)
        
        # Penalize manual interventions
        manual_conflicts = sum(1 for c in recent_conflicts if c.manual_intervention)
        manual_penalty = (manual_conflicts / len(recent_conflicts)) * 20
        
        return max(0, (resolution_rate * 100) - manual_penalty)
    
    def _calculate_performance_score(self, recent_metrics: List[CRDTHealthMetrics]) -> float:
        """Calculate performance health score"""
        if not recent_metrics:
            return 100.0
        
        avg_performance_impact = statistics.mean(m.performance_impact_percent for m in recent_metrics)
        
        # Score decreases as performance impact increases
        if avg_performance_impact <= 5:
            return 100.0
        elif avg_performance_impact <= 15:
            return 90.0
        elif avg_performance_impact <= 30:
            return 75.0
        else:
            return max(50.0, 100 - avg_performance_impact)
    
    def _calculate_consistency_score(self, recent_metrics: List[CRDTHealthMetrics]) -> float:
        """Calculate data consistency health score"""
        if not recent_metrics:
            return 100.0
        
        avg_consistency = statistics.mean(m.data_consistency_score for m in recent_metrics)
        return avg_consistency * 100
    
    def get_sync_performance_trend(self, hours: int = 24) -> Dict[str, Any]:
        """Get synchronization performance trend"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        recent_syncs = [s for s in self.sync_metrics if s.timestamp > cutoff_time]
        
        if not recent_syncs:
            return {"error": "No sync data available"}
        
        successful_syncs = [s for s in recent_syncs if s.success]
        failed_syncs = [s for s in recent_syncs if not s.success]
        
        return {
            "total_syncs": len(recent_syncs),
            "successful_syncs": len(successful_syncs),
            "failed_syncs": len(failed_syncs),
            "success_rate": len(successful_syncs) / len(recent_syncs) if recent_syncs else 0,
            "average_duration_ms": statistics.mean(s.sync_duration_ms for s in successful_syncs) if successful_syncs else 0,
            "total_bandwidth_mb": sum(s.bandwidth_used_bytes for s in recent_syncs) / 1024 / 1024,
            "average_compression_ratio": statistics.mean(s.compression_ratio for s in recent_syncs) if recent_syncs else 1.0
        }
    
    def get_conflict_analysis(self, hours: int = 24) -> Dict[str, Any]:
        """Get conflict analysis and trends"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        recent_conflicts = [c for c in self.conflict_metrics if c.detection_time > cutoff_time]
        
        if not recent_conflicts:
            return {"total_conflicts": 0, "conflict_types": {}, "resolution_strategies": {}}
        
        # Analyze conflict types
        conflict_type_counts = defaultdict(int)
        for conflict in recent_conflicts:
            conflict_type_counts[conflict.conflict_type] += 1
        
        # Analyze resolution strategies
        strategy_counts = defaultdict(int)
        for conflict in recent_conflicts:
            strategy_counts[conflict.resolution_strategy] += 1
        
        # Calculate resolution times
        resolved_conflicts = [c for c in recent_conflicts if c.resolution_time and c.resolution_duration_ms]
        avg_resolution_time = statistics.mean(c.resolution_duration_ms for c in resolved_conflicts) if resolved_conflicts else 0
        
        return {
            "total_conflicts": len(recent_conflicts),
            "resolved_conflicts": len(resolved_conflicts),
            "resolution_rate": len(resolved_conflicts) / len(recent_conflicts) if recent_conflicts else 0,
            "average_resolution_time_ms": avg_resolution_time,
            "conflict_types": dict(conflict_type_counts),
            "resolution_strategies": dict(strategy_counts),
            "manual_interventions": sum(1 for c in recent_conflicts if c.manual_intervention)
        }


class CRDTDashboardExtension:
    """Extended dashboard with CRDT monitoring capabilities"""
    
    def __init__(self, crdt_manager=None):
        self.crdt_manager = crdt_manager
        self.metrics_collector = CRDTMetricsCollector()
        self.monitoring_active = False
        self.monitoring_thread = None
        
    def start_monitoring(self):
        """Start CRDT monitoring"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(target=self._monitoring_worker, daemon=True)
            self.monitoring_thread.start()
            logger.info("CRDT dashboard monitoring started")
    
    def stop_monitoring(self):
        """Stop CRDT monitoring"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logger.info("CRDT dashboard monitoring stopped")
    
    def generate_crdt_health_report(self) -> Dict[str, Any]:
        """Generate comprehensive CRDT health report"""
        current_time = datetime.utcnow()
        
        # Collect current metrics if CRDT manager is available
        if self.crdt_manager:
            current_metrics = self._collect_current_metrics()
            self.metrics_collector.record_health_metrics(current_metrics)
        
        # Calculate health scores
        overall_health = self.metrics_collector.get_current_health_score()
        sync_trend = self.metrics_collector.get_sync_performance_trend()
        conflict_analysis = self.metrics_collector.get_conflict_analysis()
        
        return {
            "timestamp": current_time.isoformat(),
            "overall_health_score": overall_health,
            "health_status": self._get_health_status(overall_health),
            "sync_performance": sync_trend,
            "conflict_analysis": conflict_analysis,
            "recommendations": self._generate_recommendations(overall_health, sync_trend, conflict_analysis),
            "network_topology": self._get_network_topology(),
            "performance_impact": self._calculate_performance_impact()
        }
    
    def _collect_current_metrics(self) -> CRDTHealthMetrics:
        """Collect current CRDT metrics"""
        # This would integrate with actual CRDT manager
        # For now, return simulated metrics
        return CRDTHealthMetrics(
            timestamp=datetime.utcnow(),
            sync_status="active",
            active_peers=0,  # Would get from network manager
            total_operations=1000,  # Would get from CRDT manager
            successful_syncs=95,
            failed_syncs=5,
            conflicts_detected=10,
            conflicts_resolved=9,
            average_sync_time_ms=150.0,
            network_partition_resilience=0.95,
            data_consistency_score=0.98,
            performance_impact_percent=8.5
        )
    
    def _get_health_status(self, health_score: float) -> str:
        """Convert health score to status"""
        if health_score >= 95:
            return "EXCELLENT"
        elif health_score >= 85:
            return "GOOD"
        elif health_score >= 70:
            return "WARNING"
        else:
            return "CRITICAL"
    
    def _generate_recommendations(self, health_score: float, sync_trend: Dict, conflict_analysis: Dict) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Health-based recommendations
        if health_score < 85:
            recommendations.append("Overall health below optimal - investigate sync and conflict issues")
        
        # Sync-based recommendations
        if sync_trend.get("success_rate", 1.0) < 0.9:
            recommendations.append("Sync success rate low - check network connectivity and peer health")
        
        if sync_trend.get("average_duration_ms", 0) > 1000:
            recommendations.append("Sync operations slow - consider enabling compression or reducing batch sizes")
        
        # Conflict-based recommendations
        if conflict_analysis.get("resolution_rate", 1.0) < 0.8:
            recommendations.append("Conflict resolution rate low - review resolution strategies")
        
        if conflict_analysis.get("manual_interventions", 0) > 5:
            recommendations.append("High manual interventions - consider updating automatic resolution rules")
        
        # Bandwidth recommendations
        if sync_trend.get("total_bandwidth_mb", 0) > 100:
            recommendations.append("High bandwidth usage - enable delta compression")
        
        if not recommendations:
            recommendations.append("System operating optimally - no immediate actions required")
        
        return recommendations
    
    def _get_network_topology(self) -> Dict[str, Any]:
        """Get current network topology information"""
        # Placeholder for network topology visualization
        return {
            "nodes": 1,
            "connections": 0,
            "partition_groups": 1,
            "cluster_health": "stable"
        }
    
    def _calculate_performance_impact(self) -> Dict[str, Any]:
        """Calculate CRDT performance impact"""
        return {
            "baseline_operations_per_second": 1000,
            "current_operations_per_second": 920,
            "performance_degradation_percent": 8.0,
            "memory_overhead_mb": 15.2,
            "cpu_overhead_percent": 3.5
        }
    
    def _monitoring_worker(self):
        """Background monitoring worker"""
        while self.monitoring_active:
            try:
                # Collect and record metrics
                if self.crdt_manager:
                    metrics = self._collect_current_metrics()
                    self.metrics_collector.record_health_metrics(metrics)
                
                # Log periodic health summary
                health_score = self.metrics_collector.get_current_health_score()
                logger.debug(f"CRDT Health Score: {health_score:.1f}%")
                
            except Exception as e:
                logger.error(f"CRDT monitoring error: {e}")
            
            time.sleep(30)  # Monitor every 30 seconds
    
    def export_metrics(self, format_type: str = "json", hours: int = 24) -> str:
        """Export metrics in specified format"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        # Collect relevant metrics
        health_data = [asdict(m) for m in self.metrics_collector.health_metrics 
                      if m.timestamp > cutoff_time]
        sync_data = [asdict(m) for m in self.metrics_collector.sync_metrics 
                    if m.timestamp > cutoff_time]
        conflict_data = [asdict(m) for m in self.metrics_collector.conflict_metrics 
                        if m.detection_time > cutoff_time]
        
        export_data = {
            "export_timestamp": datetime.utcnow().isoformat(),
            "time_range_hours": hours,
            "health_metrics": health_data,
            "sync_metrics": sync_data,
            "conflict_metrics": conflict_data,
            "summary": {
                "total_health_records": len(health_data),
                "total_sync_records": len(sync_data),
                "total_conflict_records": len(conflict_data)
            }
        }
        
        if format_type.lower() == "json":
            return json.dumps(export_data, indent=2, default=str)
        else:
            # Could add CSV, XML, or other formats
            return json.dumps(export_data, indent=2, default=str)


class CRDTAlerting:
    """Alerting system for CRDT health monitoring"""
    
    def __init__(self):
        self.alert_rules = []
        self.alert_handlers = []
        self.alert_history = deque(maxlen=1000)
        self.cooldown_periods = {}
        
    def add_alert_rule(self, name: str, condition_func: callable, severity: str, cooldown_minutes: int = 5):
        """Add a new alert rule"""
        self.alert_rules.append({
            'name': name,
            'condition': condition_func,
            'severity': severity,
            'cooldown_minutes': cooldown_minutes
        })
    
    def add_alert_handler(self, handler_func: callable):
        """Add an alert handler"""
        self.alert_handlers.append(handler_func)
    
    def check_alerts(self, metrics: CRDTHealthMetrics):
        """Check all alert rules against current metrics"""
        current_time = datetime.utcnow()
        
        for rule in self.alert_rules:
            try:
                # Check cooldown
                if rule['name'] in self.cooldown_periods:
                    last_alert_time = self.cooldown_periods[rule['name']]
                    if current_time - last_alert_time < timedelta(minutes=rule['cooldown_minutes']):
                        continue
                
                # Check condition
                if rule['condition'](metrics):
                    self._trigger_alert(rule, metrics, current_time)
                    
            except Exception as e:
                logger.error(f"Alert rule '{rule['name']}' failed: {e}")
    
    def _trigger_alert(self, rule: Dict, metrics: CRDTHealthMetrics, timestamp: datetime):
        """Trigger an alert"""
        alert = {
            'timestamp': timestamp,
            'rule_name': rule['name'],
            'severity': rule['severity'],
            'metrics': asdict(metrics),
            'message': f"CRDT Alert: {rule['name']} triggered"
        }
        
        # Record alert
        self.alert_history.append(alert)
        self.cooldown_periods[rule['name']] = timestamp
        
        # Send to handlers
        for handler in self.alert_handlers:
            try:
                handler(alert)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
        
        logger.warning(f"CRDT Alert: {rule['name']} ({rule['severity']})")
    
    def setup_default_alerts(self):
        """Setup default alert rules"""
        # Sync failure rate alert
        self.add_alert_rule(
            name="high_sync_failure_rate",
            condition_func=lambda m: (m.failed_syncs / max(1, m.successful_syncs + m.failed_syncs)) > 0.2,
            severity="high",
            cooldown_minutes=10
        )
        
        # Performance degradation alert
        self.add_alert_rule(
            name="performance_degradation",
            condition_func=lambda m: m.performance_impact_percent > 25,
            severity="medium",
            cooldown_minutes=15
        )
        
        # Consistency issues alert
        self.add_alert_rule(
            name="data_consistency_low",
            condition_func=lambda m: m.data_consistency_score < 0.9,
            severity="high",
            cooldown_minutes=5
        )
        
        # High conflict rate alert
        self.add_alert_rule(
            name="high_conflict_rate",
            condition_func=lambda m: m.conflicts_detected > 20,
            severity="medium",
            cooldown_minutes=30
        )


def default_alert_handler(alert: Dict):
    """Default alert handler that logs alerts"""
    severity_emoji = {
        'low': 'ðŸŸ¡',
        'medium': 'ðŸŸ ', 
        'high': 'ðŸ”´',
        'critical': 'ðŸ’¥'
    }
    
    emoji = severity_emoji.get(alert['severity'], 'âš ï¸')
    logger.warning(f"{emoji} {alert['message']} (Severity: {alert['severity']})")


class CRDTMonitoringCoordinator:
    """Main coordinator for CRDT monitoring and observability"""
    
    def __init__(self, crdt_manager=None):
        self.dashboard = CRDTDashboardExtension(crdt_manager)
        self.alerting = CRDTAlerting()
        self.running = False
        
        # Setup default configuration
        self.alerting.setup_default_alerts()
        self.alerting.add_alert_handler(default_alert_handler)
    
    def start(self):
        """Start all monitoring components"""
        if not self.running:
            self.running = True
            self.dashboard.start_monitoring()
            logger.info("CRDT monitoring coordinator started")
    
    def stop(self):
        """Stop all monitoring components"""
        if self.running:
            self.running = False
            self.dashboard.stop_monitoring()
            logger.info("CRDT monitoring coordinator stopped")
    
    def get_comprehensive_status(self) -> Dict[str, Any]:
        """Get comprehensive CRDT status"""
        health_report = self.dashboard.generate_crdt_health_report()
        
        # Add alerting information
        health_report['alerting'] = {
            'active_rules': len(self.alerting.alert_rules),
            'recent_alerts': len([a for a in self.alerting.alert_history 
                                if a['timestamp'] > datetime.utcnow() - timedelta(hours=24)])
        }
        
        return health_report
    
    def simulate_metrics_for_testing(self):
        """Simulate metrics for testing purposes"""
        # Simulate some sync metrics
        for i in range(10):
            sync_metrics = SyncMetrics(
                peer_node=f"test_peer_{i}",
                sync_duration_ms=100 + (i * 20),
                operations_sent=50 + i,
                operations_received=45 + i,
                bandwidth_used_bytes=1024 * (i + 1),
                compression_ratio=1.5 + (i * 0.1),
                success=i < 8,  # 2 failures
                timestamp=datetime.utcnow() - timedelta(minutes=i),
                error_message="Network timeout" if i >= 8 else None
            )
            self.dashboard.metrics_collector.record_sync_metrics(sync_metrics)
        
        # Simulate some conflict metrics
        for i in range(5):
            conflict_metrics = ConflictMetrics(
                conflict_id=f"conflict_{i}",
                conflict_type="semantic" if i % 2 == 0 else "business_logic",
                detection_time=datetime.utcnow() - timedelta(minutes=i * 5),
                resolution_time=datetime.utcnow() - timedelta(minutes=i * 5 - 2),
                resolution_strategy="last_write_wins" if i < 3 else "manual_intervention",
                involved_nodes=[f"node_{i}", f"node_{i+1}"],
                resolution_duration_ms=200 + (i * 50),
                success=i < 4,  # 1 failure
                manual_intervention=i >= 3
            )
            self.dashboard.metrics_collector.record_conflict_metrics(conflict_metrics)